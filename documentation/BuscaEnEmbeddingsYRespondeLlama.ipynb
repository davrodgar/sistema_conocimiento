{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Respuesta generada:\n",
      " La información proporcionada indica que el HRP (Humanitarian Response Plan) de Burkina Faso está bajofinanciado, y hay dudas sobre si un proyecto piloto de acción anticipatorio puede desviar fondos necesarios para necesidades más urgentes identificadas por el Cluster de Seguridad Alimentaria. La pregunta sugerida es: ¿Cuál podría ser el impacto potencial de $15 millones en la región o a nivel global, y cómo se compara con los impactos del enfoque sin riesgos?\n",
      "\n",
      "📌 Referencias utilizadas:\n",
      "1e8c2332a461a3a142840fa477fa907c66c35dac (distancia: 0.5305)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import ollama  # Importar la API de Ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Parámetros de filtrado\n",
    "UMBRAL_BASE = 0.60  \n",
    "UMBRAL_MAXIMO = 0.45  \n",
    "MIN_DOCUMENTOS_RELEVANTES = 1\n",
    "\n",
    "def cargar_embeddings_desde_archivo(ruta_archivo):\n",
    "    \"\"\"Carga los embeddings desde un archivo JSON.\"\"\"\n",
    "    with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"archivo\"], data[\"secciones\"]\n",
    "\n",
    "def calcular_similitud(embedding1, embedding2):\n",
    "    \"\"\"Calcula la similitud mediante la distancia del coseno.\"\"\"\n",
    "    return cosine(embedding1, embedding2)\n",
    "\n",
    "def buscar_documentos_similares(texto, carpeta_embeddings):\n",
    "    \"\"\"\n",
    "    Busca documentos en una carpeta que sean más similares al texto de entrada.\n",
    "    Retorna una lista con el nombre del archivo, la menor distancia y el párrafo más similar.\n",
    "    \"\"\"\n",
    "    embedding_texto = embedding_model.encode(texto)\n",
    "    resultados = {}\n",
    "\n",
    "    for archivo in os.listdir(carpeta_embeddings):\n",
    "        if archivo.endswith(\"_embeddings.json\"):\n",
    "            ruta_archivo = os.path.join(carpeta_embeddings, archivo)\n",
    "            nombre_archivo, secciones = cargar_embeddings_desde_archivo(ruta_archivo)\n",
    "\n",
    "            menor_distancia = float(\"inf\")\n",
    "            parrafo_mas_similar = \"\"\n",
    "\n",
    "            for seccion in secciones:\n",
    "                embedding_parrafo = np.array(seccion[\"embedding\"])\n",
    "                distancia = calcular_similitud(embedding_texto, embedding_parrafo)\n",
    "\n",
    "                if distancia < menor_distancia:\n",
    "                    menor_distancia = distancia\n",
    "                    parrafo_mas_similar = seccion[\"texto\"]\n",
    "\n",
    "            resultados[nombre_archivo] = (menor_distancia, parrafo_mas_similar)\n",
    "\n",
    "    resultados_ordenados = sorted(resultados.items(), key=lambda x: x[1][0])\n",
    "\n",
    "    umbral_actual = UMBRAL_BASE\n",
    "    documentos_relevantes = [(archivo, distancia, parrafo) for archivo, (distancia, parrafo) in resultados_ordenados if distancia <= umbral_actual]\n",
    "\n",
    "    if len(documentos_relevantes) < MIN_DOCUMENTOS_RELEVANTES:\n",
    "        umbral_actual = UMBRAL_MAXIMO\n",
    "        documentos_relevantes = [(archivo, distancia, parrafo) for archivo, (distancia, parrafo) in resultados_ordenados if distancia <= umbral_actual]\n",
    "\n",
    "    if len(documentos_relevantes) < MIN_DOCUMENTOS_RELEVANTES:\n",
    "        return []\n",
    "\n",
    "    return documentos_relevantes\n",
    "\n",
    "def generar_respuesta_con_ollama(texto_pregunta, carpeta_embeddings, modelo_ollama=\"mistral\"):\n",
    "    \"\"\"\n",
    "    Genera una respuesta en lenguaje natural a partir de los párrafos más similares,\n",
    "    utilizando Ollama como modelo generativo e incluyendo referencias a los documentos originales.\n",
    "    \"\"\"\n",
    "    documentos_relevantes = buscar_documentos_similares(texto_pregunta, carpeta_embeddings)\n",
    "\n",
    "    if not documentos_relevantes:\n",
    "        return \"No se encontró una respuesta clara en los documentos.\"\n",
    "\n",
    "    # Construir el prompt para Ollama\n",
    "    contexto = \"A continuación, se presentan extractos de documentos relevantes:\\n\\n\"\n",
    "    referencias = []\n",
    "\n",
    "    for archivo, distancia, parrafo in documentos_relevantes[:3]:  # Tomar hasta 3 párrafos relevantes\n",
    "        contexto += f\"- {parrafo}\\n\\n\"\n",
    "        referencias.append(f\"{archivo} (distancia: {distancia:.4f})\")\n",
    "\n",
    "    contexto += f\"\\nPregunta: {texto_pregunta}\\n\"\n",
    "    contexto += \"Por favor, genera una respuesta concisa basada en la información proporcionada.\"\n",
    "\n",
    "    # Enviar el prompt a Ollama\n",
    "    respuesta_ollama = ollama.chat(model=modelo_ollama, messages=[{\"role\": \"user\", \"content\": contexto}])\n",
    "\n",
    "    # Extraer solo el contenido de la respuesta\n",
    "    respuesta_generada = respuesta_ollama[\"message\"][\"content\"]\n",
    "\n",
    "    # Incluir referencias en la salida\n",
    "    respuesta_final = f\"{respuesta_generada}\\n\\n📌 Referencias utilizadas:\\n\" + \"\\n\".join(referencias)\n",
    "\n",
    "    return respuesta_final\n",
    "\n",
    "# Ejemplo de uso\n",
    "carpeta_embeddings = \"data\"\n",
    "texto_pregunta = \"All the information about Burkina?\"\n",
    "\n",
    "respuesta = generar_respuesta_con_ollama(texto_pregunta, carpeta_embeddings, modelo_ollama=\"mistral\")\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(\"\\n🔹 Respuesta generada:\")\n",
    "print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
