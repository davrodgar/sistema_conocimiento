{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Procesando: 0e21835a42a6df2405496f62647058ff855743c1\n",
      "üìù Texto extra√≠do guardado en: data\\0e21835a42a6df2405496f62647058ff855743c1.txt\n",
      "üîπ Se detectaron 21 secciones en 0e21835a42a6df2405496f62647058ff855743c1\n",
      "‚úÖ Procesamiento de 0e21835a42a6df2405496f62647058ff855743c1 completado. Datos almacenados en data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar modelos\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"PlanTL-GOB-ES/roberta-base-bne\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Directorios de almacenamiento\n",
    "DATA_DIR = \"data\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "# üîπ 1. Extraer texto de PDF\n",
    "def extraer_texto_pdf(ruta_pdf):\n",
    "    texto_completo = \"\"\n",
    "    with pdfplumber.open(ruta_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            texto_pagina = pagina.extract_text(x_tolerance=1, y_tolerance=3)\n",
    "            if texto_pagina:\n",
    "                texto_completo += texto_pagina.strip() + \"\\n\\n\"  # Agregar doble salto de l√≠nea para separar p√°ginas correctamente\n",
    "    return texto_completo.strip()\n",
    "\n",
    "# üîπ 2. Segmentar texto en p√°rrafos\n",
    "def segmentar_en_parrafos(texto):\n",
    " #   parrafos = re.split(r\"\\n\\s*\\n\", texto.strip())  # Separar por dobles saltos de l√≠nea\n",
    " #   return [p.strip() for p in parrafos if len(p.strip()) > 30]  # Evitar fragmentos cortos\n",
    " #   Versi√≥n 1 -> codigo anterior -> Separa por p√°ginas -> cambiamos a nueva versi√≥n\n",
    "    \"\"\"\n",
    "    Separa el texto en p√°rrafos usando:\n",
    "    - Dobles saltos de l√≠nea\n",
    "    - Punto seguido de salto de l√≠nea y una may√∫scula (inicio de un nuevo p√°rrafo)\n",
    "    \"\"\"\n",
    "    # Normalizar saltos de l√≠nea\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)  # Reemplazar m√∫ltiples saltos por uno solo\n",
    "\n",
    "    # Separar por dobles saltos de l√≠nea o punto seguido de un salto de l√≠nea y may√∫scula\n",
    "    parrafos = re.split(r\"\\n\\s*\\n|(?<=\\.)\\n(?=[A-Z])\", texto)\n",
    "\n",
    "    # Filtrar fragmentos muy cortos\n",
    "    parrafos = [p.strip() for p in parrafos if len(p.strip()) > 30]\n",
    "\n",
    "    return parrafos\n",
    "\n",
    "# üîπ 3. Extraer entidades clave (NER)\n",
    "import spacy\n",
    "\n",
    "# Cargar modelo NER en espa√±ol m√°s robusto\n",
    "# nlp = spacy.load(\"es_core_news_sm\") -> con este modelo los resultados son muy malos\n",
    "\n",
    "# El archivo generado con las entidades clave (NER) contiene resultados err√≥neos. üìâ\n",
    "\n",
    "# üìå Problemas observados:\n",
    "# ‚úÖ Las entidades detectadas est√°n mal segmentadas.\n",
    "# ‚úÖ Muchas entidades parecen fragmentos de palabras en lugar de t√©rminos completos.\n",
    "# ‚úÖ Los tipos de entidades (LABEL_0, LABEL_1) no tienen significado √∫til.\n",
    "# ‚úÖ Errores en la tokenizaci√≥n, con cortes extra√±os en palabras.\n",
    "# An√°lisis de las Causas del Problema\n",
    "# 1Ô∏è‚É£ El modelo NER usado (PlanTL-GOB-ES/roberta-base-bne) no est√° funcionando bien con el texto extra√≠do.\n",
    "# 2Ô∏è‚É£ El texto del documento podr√≠a estar mal segmentado, afectando la detecci√≥n de entidades.\n",
    "# 3Ô∏è‚É£ El modelo NER usado requiere una preprocesamiento diferente (quiz√°s eliminar saltos de l√≠nea, caracteres especiales, etc.).\n",
    "# 4Ô∏è‚É£ Las entidades detectadas no est√°n bien clasificadas en PERSON, ORG, LOC, etc., lo que indica un problema con el pipeline(\"ner\").\n",
    "# Realizados los cambios\n",
    "# Cargar modelo NER en espa√±ol m√°s robusto\n",
    "# limpiar_texto + nlp = spacy.load(\"es_core_news_sm\")\n",
    "# Problemas Detectados\n",
    "# ‚úÖ Clasificaci√≥n incorrecta de entidades ‚Üí PER (Persona) asignado a t√©rminos como \"Model Report\" o \"Make\".\n",
    "# ‚úÖ Demasiadas entidades MISC sin valor informativo.\n",
    "# ‚úÖ Entidades mal segmentadas y con palabras fragmentadas.\n",
    "# ‚úÖ Poca detecci√≥n de entidades de tipo ORG (Organizaci√≥n) o LOC (Ubicaci√≥n).\n",
    "\n",
    "# 2Ô∏è‚É£ Si los resultados a√∫n tienen problemas, considerar otro modelo (es_core_news_lg).\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_lg\") -> no funciona bien es un modelo para texto en espa√±ol\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando saltos de l√≠nea innecesarios y caracteres extra√±os.\n",
    "    \"\"\"\n",
    "    texto = texto.replace(\"\\n\", \" \")  # Reemplazar saltos de l√≠nea con espacios\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()  # Eliminar espacios dobles\n",
    "    return texto\n",
    "\n",
    "\n",
    "def extraer_entidades(parrafo):\n",
    " #   entidades = ner_pipeline(parrafo)\n",
    " #   return [{\"texto\": ent[\"word\"], \"tipo\": ent[\"entity_group\"]} for ent in entidades]\n",
    " \n",
    " #  Extrae entidades clave (NER) usando spaCy.\n",
    " \n",
    "    parrafo = limpiar_texto(parrafo)  # Limpiar el texto antes de pasarlo al NER\n",
    "    doc = nlp(parrafo)\n",
    "    entidades = [\n",
    "        {\"texto\": ent.text, \"tipo\": ent.label_}\n",
    "        for ent in doc.ents\n",
    "        if ent.label_ in [\"PER\", \"ORG\", \"LOC\", \"MISC\", \"DATE\"]\n",
    "    ]  # Filtrar solo entidades √∫tiles\n",
    "\n",
    "    return entidades\n",
    "\n",
    "# üîπ 4. Calcular embeddings\n",
    "def calcular_embeddings(secciones):\n",
    "    return embedding_model.encode(secciones)\n",
    "\n",
    "# üîπ 5. Procesar un documento\n",
    "def procesar_documento(ruta_pdf):\n",
    "    # Obtener nombre del archivo sin extensi√≥n\n",
    "    nombre_archivo = os.path.splitext(os.path.basename(ruta_pdf))[0]\n",
    "\n",
    "    print(f\"üìÇ Procesando: {nombre_archivo}\")\n",
    "\n",
    "    # Extraer texto\n",
    "    texto = extraer_texto_pdf(ruta_pdf)\n",
    "    if not texto:\n",
    "        print(f\"‚ö†Ô∏è No se pudo extraer texto de {nombre_archivo}\")\n",
    "        return\n",
    "\n",
    "    # Guardar texto extra√≠do en un archivo .txt\n",
    "    ruta_txt = os.path.join(DATA_DIR, f\"{nombre_archivo}.txt\")\n",
    "    with open(ruta_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(texto)\n",
    "    print(f\"üìù Texto extra√≠do guardado en: {ruta_txt}\")\n",
    "\n",
    "\n",
    "    # Segmentar en secciones\n",
    "    secciones = segmentar_en_parrafos(texto)\n",
    "    print(f\"üîπ Se detectaron {len(secciones)} secciones en {nombre_archivo}\")\n",
    "\n",
    "    # Calcular embeddings de cada secci√≥n\n",
    "    embeddings = calcular_embeddings(secciones)\n",
    "\n",
    "    # Extraer entidades clave de cada secci√≥n\n",
    "    entidades_totales = {}\n",
    "    for idx, seccion in enumerate(secciones):\n",
    "        entidades = extraer_entidades(seccion)\n",
    "        entidades_totales[f\"seccion_{idx+1}\"] = entidades\n",
    "\n",
    "    # Guardar embeddings en archivo JSON\n",
    "    embeddings_json = {\n",
    "        \"archivo\": nombre_archivo,\n",
    "        \"secciones\": [{\"id\": f\"seccion_{i+1}\", \"texto\": sec, \"embedding\": emb.tolist()} for i, (sec, emb) in enumerate(zip(secciones, embeddings))]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, f\"{nombre_archivo}_embeddings.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(embeddings_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Guardar entidades en archivo JSON\n",
    "    entidades_json = {\n",
    "        \"archivo\": nombre_archivo,\n",
    "        \"entidades\": entidades_totales\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, f\"{nombre_archivo}_entidades.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(entidades_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Procesamiento de {nombre_archivo} completado. Datos almacenados en {DATA_DIR}/\")\n",
    "\n",
    "# üîπ 6. Procesar documentos de una carpeta\n",
    "if __name__ == \"__main__\":\n",
    "    directorio_documentos = \"./documentos\"\n",
    "    \n",
    "    for archivo in os.listdir(directorio_documentos):\n",
    "        if archivo.endswith(\".pdf\"):\n",
    "            procesar_documento(os.path.join(directorio_documentos, archivo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_TFM2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
